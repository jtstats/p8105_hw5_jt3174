p8105\_hw5\_jt3174
================
Jingyi
11/6/2019

# Problem 1

``` r
# load data as copied from Jeff
iris_with_missing = iris %>% 
  map_df(~replace(.x, sample(1:150, 20), NA)) %>%
  mutate(Species = as.character(Species))
```

``` r
# write a function that fill all missing numeric variables with the mean of non-missing values and all missing categorical variables with virginica

filling = function(column) {
  # make sure the argument is a vector, and if not, stop
  if (!is.vector(column)) {
    stop("Argument should be a vector.")
  }
  # to distinguish numeric variable 
  if (is.numeric(column)) {
    # to make sure we have all orginal data in the output
    column = column %>% 
    # to replace all missing data with the mean of what are not missing in the vector argument
    replace_na(mean(column[!is.na(column)])) 
  }
  # for all other left variables, dintinguish whether this is a categorical variable
  else if (is.character(column)) {
    # make sure we have all original data in the output
    column = column %>% 
    # to replace all missing data with virginica
    str_replace_na("virginica") 
  }
  
}

# use map to iterate over each variable of the dataframe iris_with_missing with filling function that I just wrote above and return a dataframe called iris_filled
iris_filled = map_df(iris_with_missing, filling)
```

# Problem 2

``` r
# vector for pathes of files
path = list.files(path = "./data/", full.names = TRUE)

# function for paths of files to be read and combined
read_and_combine = function(x) {
  data = read_csv(x)
  cbind(file_id = x, data)
}

# read data
arm_study = purrr::map_df(path, read_and_combine) %>% 
  # get rid of the same ./data/ in the beginning
  mutate(file_id = str_extract(file_id, "[conexp]{3}_[0-9]{2}")) %>% 
  # to get two columns: subject ID and arm
  separate(file_id, into = c("arm", "subject_id"), sep = "_") %>% 
  # reorder the variables
  select(subject_id, arm, everything())

arm_study
```

    ##    subject_id arm week_1 week_2 week_3 week_4 week_5 week_6 week_7 week_8
    ## 1          01 con   0.20  -1.31   0.66   1.96   0.23   1.09   0.05   1.94
    ## 2          02 con   1.13  -0.88   1.07   0.17  -0.83  -0.31   1.58   0.44
    ## 3          03 con   1.77   3.11   2.22   3.26   3.31   0.89   1.88   1.01
    ## 4          04 con   1.04   3.66   1.22   2.33   1.47   2.70   1.87   1.66
    ## 5          05 con   0.47  -0.58  -0.09  -1.37  -0.32  -2.17   0.45   0.48
    ## 6          06 con   2.37   2.50   1.59  -0.16   2.08   3.07   0.78   2.35
    ## 7          07 con   0.03   1.21   1.13   0.64   0.49  -0.12  -0.07   0.46
    ## 8          08 con  -0.08   1.42   0.09   0.36   1.18  -1.16   0.33  -0.44
    ## 9          09 con   0.08   1.24   1.44   0.41   0.95   2.75   0.30   0.03
    ## 10         10 con   2.14   1.15   2.52   3.44   4.26   0.97   2.73  -0.53
    ## 11         01 exp   3.05   3.67   4.84   5.80   6.33   5.46   6.38   5.91
    ## 12         02 exp  -0.84   2.63   1.64   2.58   1.24   2.32   3.11   3.78
    ## 13         03 exp   2.15   2.08   1.82   2.84   3.36   3.61   3.37   3.74
    ## 14         04 exp  -0.62   2.54   3.78   2.73   4.49   5.82   6.00   6.49
    ## 15         05 exp   0.70   3.33   5.34   5.57   6.90   6.66   6.24   6.95
    ## 16         06 exp   3.73   4.08   5.40   6.41   4.87   6.09   7.66   5.83
    ## 17         07 exp   1.18   2.35   1.23   1.17   2.02   1.61   3.13   4.88
    ## 18         08 exp   1.37   1.43   1.84   3.60   3.80   4.72   4.68   5.70
    ## 19         09 exp  -0.40   1.08   2.66   2.70   2.80   2.64   3.51   3.27
    ## 20         10 exp   1.09   2.80   2.80   4.30   2.25   6.57   6.09   4.64
